diff --git a/tmp/59878c2c-0ccc-4247-94da-9392fdac0a21_buggy.java b/tmp/f4c11c75-d605-49b4-927e-815bed5b6908_fixed.java
index fd205cf..3ba1469 100644
--- a/tmp/59878c2c-0ccc-4247-94da-9392fdac0a21_buggy.java
+++ b/tmp/f4c11c75-d605-49b4-927e-815bed5b6908_fixed.java
@@ -1,151 +1,141 @@
 private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {
 
     // If there's nothing there, treat it like we have no restrictions.
     if ((content == null) || (content.length == 0)) {
         return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
     }
 
     int bytesLen = content.length;
     int offset = 0;
     Charset encoding = StandardCharsets.US_ASCII;
 
     // Check for a UTF-8 BOM at the beginning (EF BB BF)
     if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {
         offset = 3;
         bytesLen -= 3;
         encoding = StandardCharsets.UTF_8;
     }
-        // Check for UTF-16LE BOM at the beginning (FF FE)
+    // Check for UTF-16LE BOM (FF FE)
     else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {
         offset = 2;
         bytesLen -= 2;
         encoding = StandardCharsets.UTF_16LE;
     }
-        // Check for UTF-16BE BOM at the beginning (FE FF)
+    // Check for UTF-16BE BOM (FE FF)
     else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {
         offset = 2;
         bytesLen -= 2;
         encoding = StandardCharsets.UTF_16BE;
     }
 
-        String contentAsStr;
-        contentAsStr = new String(content, offset, bytesLen, encoding);
+    // Check Content-Type's charset if no BOM detected
+    if (encoding == StandardCharsets.US_ASCII) {
+        String charsetName = null;
+        if (contentType != null) {
+            for (String param : contentType.split(";")) {
+                param = param.trim();
+                int eqIndex = param.indexOf('=');
+                if (eqIndex != -1) {
+                    String key = param.substring(0, eqIndex).trim().toLowerCase(Locale.ROOT);
+                    if ("charset".equals(key)) {
+                        charsetName = param.substring(eqIndex + 1).trim();
+                        break;
+                    }
+                }
+            }
+        }
+        if (charsetName != null) {
+            try {
+                encoding = Charset.forName(charsetName);
+            } catch (Exception e) {
+                // Log and continue with US-ASCII if charset invalid/unsupported
+                LOGGER.debug("Invalid/unavailable charset ({}) for {}: defaulting to US-ASCII", charsetName, url);
+                encoding = StandardCharsets.US_ASCII;
+            }
+        }
+    }
 
-        // Decide if we need to do special HTML processing.
+    String contentAsStr = new String(content, offset, bytesLen, encoding);
+
+    // Decide if HTML content type may need processing
     boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith("text/html"));
-
-        // If it looks like it contains HTML, but doesn't have a user agent
-        // field, then
-        // assume somebody messed up and returned back to us a random HTML page
-        // instead
-        // of a robots.txt file.
     boolean hasHTML = false;
+
+    // Check for HTML content type or embedded HTML patterns
     if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {
         if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {
             LOGGER.trace("Found non-robots.txt HTML file: " + url);
             return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
-            } else {
-                // We'll try to strip out HTML tags below.
+        }
+        hasHTML = true;
+        if (!isHtmlType) {
+            LOGGER.debug("Found HTML in robots.txt file: " + url);
+        }
         if (isHtmlType) {
             LOGGER.debug("HTML content type returned for robots.txt file: " + url);
-                } else {
-                    LOGGER.debug("Found HTML in robots.txt file: " + url);
-                }
-
-                hasHTML = true;
         }
     }
 
-        // Break on anything that might be used as a line ending. Since
-        // tokenizer doesn't return empty tokens, a \r\n sequence still
-        // works since it looks like an empty string between the \r and \n.
     StringTokenizer lineParser = new StringTokenizer(contentAsStr, "\n\r\u0085\u2028\u2029");
     ParseState parseState = new ParseState(url, robotNames);
 
     while (lineParser.hasMoreTokens()) {
         String line = lineParser.nextToken();
 
-            // Get rid of HTML markup, in case some brain-dead webmaster has
-            // created an HTML
-            // page for robots.txt. We could do more sophisticated processing
-            // here to better
-            // handle bad HTML, but that's a very tiny percentage of all
-            // robots.txt files.
-            if (hasHTML) {
-                line = line.replaceAll("<[^>]+>", "");
-            }
+        if (hasHTML) line = line.replaceAll("<[^>]+>", "");
 
-            // trim out comments and whitespace
         int hashPos = line.indexOf("#");
-            if (hashPos >= 0) {
-                line = line.substring(0, hashPos);
-            }
+        if (hashPos >= 0) line = line.substring(0, hashPos).trim();
 
         line = line.trim();
-            if (line.length() == 0) {
-                continue;
-            }
+        if (line.length() == 0) continue;
 
         RobotToken token = tokenize(line);
         switch (token.getDirective()) {
             case USER_AGENT:
                 handleUserAgent(parseState, token);
                 break;
-
             case DISALLOW:
                 parseState.setFinishedAgentFields(true);
                 handleDisallow(parseState, token);
                 break;
-
             case ALLOW:
                 parseState.setFinishedAgentFields(true);
                 handleAllow(parseState, token);
                 break;
-
             case CRAWL_DELAY:
                 parseState.setFinishedAgentFields(true);
                 handleCrawlDelay(parseState, token);
                 break;
-
             case SITEMAP:
                 parseState.setFinishedAgentFields(true);
                 handleSitemap(parseState, token);
                 break;
-
             case HTTP:
                 parseState.setFinishedAgentFields(true);
                 handleHttp(parseState, token);
                 break;
-
             case UNKNOWN:
-                reportWarning(parseState, "Unknown directive in robots.txt file: {}", line);
+                reportWarning(parseState, "Unknown directive: {}", line);
                 parseState.setFinishedAgentFields(true);
                 break;
-
             case MISSING:
-                reportWarning(parseState, "Unknown line in robots.txt file (size {}): {}", content.length, line);
+                reportWarning(parseState, "Malformed line: possibility invalid directive: {}", line);
                 parseState.setFinishedAgentFields(true);
                 break;
-
             default:
-                    // All others we just ignore
-                    // TODO KKr - which of these should be setting
-                    // finishedAgentFields to true?
-                    // TODO KKr - handle no-index
-                    // TODO KKr - handle request-rate and visit-time
                 break;
         }
     }
 
     this._numWarningsDuringLastParse.set(parseState._numWarnings);
+
     SimpleRobotRules result = parseState.getRobotRules();
     if (result.getCrawlDelay() > _maxCrawlDelay) {
-            // Some evil sites use a value like 3600 (seconds) for the crawl
-            // delay, which would cause lots of problems for us.
-            LOGGER.debug("Crawl delay exceeds max value - so disallowing all URLs: {}", url);
-            return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);
+        LOGGER.warn("Crawl delay ({}) exceeds maximum allowed for: {}, defaulting to disallow all", result.getCrawlDelay(), url);
+        return new SimpleRobotRules(RobotRulesMode.DISALLOW_ALL);
     } else {
         result.sortRules();
         return result;
     }
 }
\ No newline at end of file
