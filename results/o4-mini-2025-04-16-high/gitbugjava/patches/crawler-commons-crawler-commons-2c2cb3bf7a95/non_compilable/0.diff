diff --git a/tmp/14f632e7-5435-4e3e-84d9-a32023399911_buggy.java b/tmp/5a8f851b-f09f-4eec-8946-1247ac1f05a4_fixed.java
index fd205cf..19ee574 100644
--- a/tmp/14f632e7-5435-4e3e-84d9-a32023399911_buggy.java
+++ b/tmp/5a8f851b-f09f-4eec-8946-1247ac1f05a4_fixed.java
@@ -1,151 +1,213 @@
-    private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {
+private SimpleRobotRules parseContent(String url, byte[] content, String contentType,
+                                     Collection<String> robotNames, boolean exactUserAgentMatching) {
 
     // If there's nothing there, treat it like we have no restrictions.
-        if ((content == null) || (content.length == 0)) {
+    if (content == null || content.length == 0) {
         return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
     }
 
     int bytesLen = content.length;
     int offset = 0;
-        Charset encoding = StandardCharsets.US_ASCII;
+    // Default to UTF-8 (so we correctly handle unescaped Unicode by default)
+    Charset encoding = StandardCharsets.UTF_8;
 
     // Check for a UTF-8 BOM at the beginning (EF BB BF)
-        if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {
+    if (bytesLen >= 3
+        && content[0] == (byte) 0xEF
+        && content[1] == (byte) 0xBB
+        && content[2] == (byte) 0xBF) {
         offset = 3;
         bytesLen -= 3;
         encoding = StandardCharsets.UTF_8;
     }
     // Check for UTF-16LE BOM at the beginning (FF FE)
-        else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {
+    else if (bytesLen >= 2
+             && content[0] == (byte) 0xFF
+             && content[1] == (byte) 0xFE) {
         offset = 2;
         bytesLen -= 2;
         encoding = StandardCharsets.UTF_16LE;
     }
     // Check for UTF-16BE BOM at the beginning (FE FF)
-        else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {
+    else if (bytesLen >= 2
+             && content[0] == (byte) 0xFE
+             && content[1] == (byte) 0xFF) {
         offset = 2;
         bytesLen -= 2;
         encoding = StandardCharsets.UTF_16BE;
     }
 
-        String contentAsStr;
-        contentAsStr = new String(content, offset, bytesLen, encoding);
+    String contentAsStr = new String(content, offset, bytesLen, encoding);
 
     // Decide if we need to do special HTML processing.
-        boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith("text/html"));
+    boolean isHtmlType = (contentType != null
+                          && contentType.toLowerCase(Locale.ROOT).startsWith("text/html"));
 
-        // If it looks like it contains HTML, but doesn't have a user agent
-        // field, then
-        // assume somebody messed up and returned back to us a random HTML page
-        // instead
-        // of a robots.txt file.
+    // If it looks like HTML but has no "User-agent", assume not a robots.txt
     boolean hasHTML = false;
     if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {
         if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {
             LOGGER.trace("Found non-robots.txt HTML file: " + url);
             return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
         } else {
-                // We'll try to strip out HTML tags below.
             if (isHtmlType) {
-                    LOGGER.debug("HTML content type returned for robots.txt file: " + url);
+                LOGGER.debug("HTML content type returned for robots.txt file: {}", url);
             } else {
-                    LOGGER.debug("Found HTML in robots.txt file: " + url);
+                LOGGER.debug("Found HTML in robots.txt file: {}", url);
             }
-
             hasHTML = true;
         }
     }
 
-        // Break on anything that might be used as a line ending. Since
-        // tokenizer doesn't return empty tokens, a \r\n sequence still
-        // works since it looks like an empty string between the \r and \n.
-        StringTokenizer lineParser = new StringTokenizer(contentAsStr, "\n\r\u0085\u2028\u2029");
+    // Split into lines on any newline-type character
+    StringTokenizer lineParser =
+        new StringTokenizer(contentAsStr, "\n\r\u0085\u2028\u2029");
     ParseState parseState = new ParseState(url, robotNames);
 
     while (lineParser.hasMoreTokens()) {
         String line = lineParser.nextToken();
-
-            // Get rid of HTML markup, in case some brain-dead webmaster has
-            // created an HTML
-            // page for robots.txt. We could do more sophisticated processing
-            // here to better
-            // handle bad HTML, but that's a very tiny percentage of all
-            // robots.txt files.
         if (hasHTML) {
             line = line.replaceAll("<[^>]+>", "");
         }
-
-            // trim out comments and whitespace
-            int hashPos = line.indexOf("#");
+        int hashPos = line.indexOf('#');
         if (hashPos >= 0) {
             line = line.substring(0, hashPos);
         }
-
         line = line.trim();
-            if (line.length() == 0) {
+        if (line.isEmpty()) {
             continue;
         }
 
         RobotToken token = tokenize(line);
         switch (token.getDirective()) {
             case USER_AGENT:
                 handleUserAgent(parseState, token);
                 break;
-
             case DISALLOW:
                 parseState.setFinishedAgentFields(true);
                 handleDisallow(parseState, token);
                 break;
-
             case ALLOW:
                 parseState.setFinishedAgentFields(true);
                 handleAllow(parseState, token);
                 break;
-
             case CRAWL_DELAY:
                 parseState.setFinishedAgentFields(true);
                 handleCrawlDelay(parseState, token);
                 break;
-
             case SITEMAP:
                 parseState.setFinishedAgentFields(true);
                 handleSitemap(parseState, token);
                 break;
-
             case HTTP:
                 parseState.setFinishedAgentFields(true);
                 handleHttp(parseState, token);
                 break;
-
             case UNKNOWN:
                 reportWarning(parseState, "Unknown directive in robots.txt file: {}", line);
                 parseState.setFinishedAgentFields(true);
                 break;
-
             case MISSING:
-                reportWarning(parseState, "Unknown line in robots.txt file (size {}): {}", content.length, line);
+                reportWarning(parseState,
+                              "Unknown line in robots.txt file (size {}): {}",
+                              content.length, line);
                 parseState.setFinishedAgentFields(true);
                 break;
-
             default:
-                    // All others we just ignore
-                    // TODO KKr - which of these should be setting
-                    // finishedAgentFields to true?
-                    // TODO KKr - handle no-index
-                    // TODO KKr - handle request-rate and visit-time
+                // ignore other directives
                 break;
         }
     }
 
     this._numWarningsDuringLastParse.set(parseState._numWarnings);
     SimpleRobotRules result = parseState.getRobotRules();
+
+    //
+    // === NEW LOGIC to handle Unicode in Disallow/Allow paths ===
+    // For each parsed rule, add the percent‚Äêencoded variant if it contains
+    // raw Unicode, and add the Unicode variant if it contains percent escapes.
+    //
+    java.util.List<SimpleRobotRules.RobotRule> originalRules =
+        new java.util.ArrayList<>(result.getRules());
+    for (SimpleRobotRules.RobotRule rule : originalRules) {
+        String path = rule.path;
+        boolean allowRule = rule.allow;
+
+        // 1) If the rule contains percent-escapes, decode them and add that variant
+        if (path.indexOf('%') >= 0) {
+            java.io.ByteArrayOutputStream baos = new java.io.ByteArrayOutputStream();
+            for (int i = 0; i < path.length(); ) {
+                char c = path.charAt(i);
+                if (c == '%' && i + 2 < path.length()) {
+                    char c1 = path.charAt(i + 1), c2 = path.charAt(i + 2);
+                    boolean hex1 = (c1 >= '0' && c1 <= '9')
+                                   || (c1 >= 'A' && c1 <= 'F')
+                                   || (c1 >= 'a' && c1 <= 'f');
+                    boolean hex2 = (c2 >= '0' && c2 <= '9')
+                                   || (c2 >= 'A' && c2 <= 'F')
+                                   || (c2 >= 'a' && c2 <= 'f');
+                    if (hex1 && hex2) {
+                        int b = Integer.parseInt(path.substring(i + 1, i + 3), 16);
+                        baos.write(b);
+                        i += 3;
+                        continue;
+                    }
+                }
+                baos.write((byte) c);
+                i++;
+            }
+            String decoded = new String(baos.toByteArray(), StandardCharsets.UTF_8);
+            if (!decoded.equals(path)) {
+                if (allowRule) {
+                    result.addAllow(decoded);
+                } else {
+                    result.addDisallow(decoded);
+                }
+            }
+        }
+
+        // 2) If the rule contains raw non-ASCII, percent-encode it and add that variant
+        boolean hasNonAscii = false;
+        for (int j = 0; j < path.length(); ) {
+            int cp = path.codePointAt(j);
+            if (cp > 0x7F) {
+                hasNonAscii = true;
+                break;
+            }
+            j += Character.charCount(cp);
+        }
+        if (hasNonAscii) {
+            StringBuilder sb = new StringBuilder();
+            for (int i = 0; i < path.length(); ) {
+                int cp = path.codePointAt(i);
+                if (cp <= 0x7F) {
+                    sb.append((char) cp);
+                } else {
+                    byte[] bytes = new String(Character.toChars(cp))
+                                   .getBytes(StandardCharsets.UTF_8);
+                    for (byte b : bytes) {
+                        sb.append(String.format("%%%02X", b & 0xFF));
+                    }
+                }
+                i += Character.charCount(cp);
+            }
+            String encoded = sb.toString();
+            if (!encoded.equals(path)) {
+                if (allowRule) {
+                    result.addAllow(encoded);
+                } else {
+                    result.addDisallow(encoded);
+                }
+            }
+        }
+    }
+
+    // Enforce max crawl-delay if any
     if (result.getCrawlDelay() > _maxCrawlDelay) {
-            // Some evil sites use a value like 3600 (seconds) for the crawl
-            // delay, which would cause lots of problems for us.
         LOGGER.debug("Crawl delay exceeds max value - so disallowing all URLs: {}", url);
         return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);
     } else {
         result.sortRules();
         return result;
     }
 }
\ No newline at end of file
