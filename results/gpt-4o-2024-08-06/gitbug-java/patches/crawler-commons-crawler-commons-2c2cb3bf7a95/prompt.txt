You are an automatic program repair tool. Your task is to fix the provided buggy code.

The following code contains a buggy function:
```java
    private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {

        // If there's nothing there, treat it like we have no restrictions.
        if ((content == null) || (content.length == 0)) {
            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
        }

        int bytesLen = content.length;
        int offset = 0;
        Charset encoding = StandardCharsets.US_ASCII;

        // Check for a UTF-8 BOM at the beginning (EF BB BF)
        if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {
            offset = 3;
            bytesLen -= 3;
            encoding = StandardCharsets.UTF_8;
        }
        // Check for UTF-16LE BOM at the beginning (FF FE)
        else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {
            offset = 2;
            bytesLen -= 2;
            encoding = StandardCharsets.UTF_16LE;
        }
        // Check for UTF-16BE BOM at the beginning (FE FF)
        else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {
            offset = 2;
            bytesLen -= 2;
            encoding = StandardCharsets.UTF_16BE;
        }

        String contentAsStr;
        contentAsStr = new String(content, offset, bytesLen, encoding);

        // Decide if we need to do special HTML processing.
        boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith("text/html"));

        // If it looks like it contains HTML, but doesn't have a user agent
        // field, then
        // assume somebody messed up and returned back to us a random HTML page
        // instead
        // of a robots.txt file.
        boolean hasHTML = false;
        if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {
            if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {
                LOGGER.trace("Found non-robots.txt HTML file: " + url);
                return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
            } else {
                // We'll try to strip out HTML tags below.
                if (isHtmlType) {
                    LOGGER.debug("HTML content type returned for robots.txt file: " + url);
                } else {
                    LOGGER.debug("Found HTML in robots.txt file: " + url);
                }

                hasHTML = true;
            }
        }

        // Break on anything that might be used as a line ending. Since
        // tokenizer doesn't return empty tokens, a \r\n sequence still
        // works since it looks like an empty string between the \r and \n.
        StringTokenizer lineParser = new StringTokenizer(contentAsStr, "\n\r\u0085\u2028\u2029");
        ParseState parseState = new ParseState(url, robotNames);

        while (lineParser.hasMoreTokens()) {
            String line = lineParser.nextToken();

            // Get rid of HTML markup, in case some brain-dead webmaster has
            // created an HTML
            // page for robots.txt. We could do more sophisticated processing
            // here to better
            // handle bad HTML, but that's a very tiny percentage of all
            // robots.txt files.
            if (hasHTML) {
                line = line.replaceAll("<[^>]+>", "");
            }

            // trim out comments and whitespace
            int hashPos = line.indexOf("#");
            if (hashPos >= 0) {
                line = line.substring(0, hashPos);
            }

            line = line.trim();
            if (line.length() == 0) {
                continue;
            }

            RobotToken token = tokenize(line);
            switch (token.getDirective()) {
                case USER_AGENT:
                handleUserAgent(parseState, token);
                    break;

                case DISALLOW:
                parseState.setFinishedAgentFields(true);
                handleDisallow(parseState, token);
                    break;

                case ALLOW:
                parseState.setFinishedAgentFields(true);
                handleAllow(parseState, token);
                    break;

                case CRAWL_DELAY:
                parseState.setFinishedAgentFields(true);
                handleCrawlDelay(parseState, token);
                    break;

                case SITEMAP:
                parseState.setFinishedAgentFields(true);
                handleSitemap(parseState, token);
                    break;

                case HTTP:
                parseState.setFinishedAgentFields(true);
                handleHttp(parseState, token);
                    break;

                case UNKNOWN:
                reportWarning(parseState, "Unknown directive in robots.txt file: {}", line);
                parseState.setFinishedAgentFields(true);
                    break;

                case MISSING:
                reportWarning(parseState, "Unknown line in robots.txt file (size {}): {}", content.length, line);
                parseState.setFinishedAgentFields(true);
                    break;

                default:
                    // All others we just ignore
                    // TODO KKr - which of these should be setting
                    // finishedAgentFields to true?
                    // TODO KKr - handle no-index
                    // TODO KKr - handle request-rate and visit-time
                    break;
            }
        }

        this._numWarningsDuringLastParse.set(parseState._numWarnings);
        SimpleRobotRules result = parseState.getRobotRules();
        if (result.getCrawlDelay() > _maxCrawlDelay) {
            // Some evil sites use a value like 3600 (seconds) for the crawl
            // delay, which would cause lots of problems for us.
            LOGGER.debug("Crawl delay exceeds max value - so disallowing all URLs: {}", url);
            return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);
        } else {
            result.sortRules();
            return result;
        }
    }

```

The code fails the following test:
```java
    @Test
    void testUnicodeUnescapedPaths() {
        final String simpleRobotsTxt = "User-agent: *" + CRLF //
                        + "Disallow: /bücher/" + CRLF //
                        + "Disallow: /k%C3%B6nyvek/" + CRLF //
                        + CRLF //
                        + "User-agent: GoodBot" + CRLF //
                        + "Allow: /";

        BaseRobotRules rules = createRobotRules("mybot", simpleRobotsTxt);
        assertTrue(rules.isAllowed("https://www.example.com/"));

        // test using escaped and unescaped URLs
        assertFalse(rules.isAllowed("https://www.example.com/b%C3%BCcher/book1.html"));
        assertFalse(rules.isAllowed("https://www.example.com/bücher/book2.html"));

        // (for completeness) check also escaped path in robots.txt
        assertFalse(rules.isAllowed("https://www.example.com/k%C3%B6nyvek/book1.html"));
        assertFalse(rules.isAllowed("https://www.example.com/könyvek/book2.html"));

        // test invalid encoding: invalid encoded characters should not break
        // parsing of rules below
        rules = createRobotRules("goodbot", simpleRobotsTxt.getBytes(StandardCharsets.ISO_8859_1));
        assertTrue(rules.isAllowed("https://www.example.com/"));
        assertTrue(rules.isAllowed("https://www.example.com/b%C3%BCcher/book1.html"));

        // test invalid encoding: only rules with invalid characters should be
        // ignored
        rules = createRobotRules("mybot", simpleRobotsTxt.getBytes(StandardCharsets.ISO_8859_1));
        assertTrue(rules.isAllowed("https://www.example.com/"));
        assertFalse(rules.isAllowed("https://www.example.com/k%C3%B6nyvek/book1.html"));
        assertFalse(rules.isAllowed("https://www.example.com/könyvek/book2.html"));
        // if URL paths in disallow rules are not properly encoded, these two
        // URLs are not matched:
        // assertFalse(rules.isAllowed("https://www.example.com/b%C3%BCcher/book2.html"));
        // assertFalse(rules.isAllowed("https://www.example.com/bücher/book1.html"));
    }

```

With the following test error:
```
expected: <false> but was: <true>
```

Please provide a fixed version of the buggy function, and only that function:
