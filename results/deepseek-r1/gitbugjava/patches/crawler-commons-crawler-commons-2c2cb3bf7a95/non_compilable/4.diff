diff --git a/tmp/cc402680-5e34-4298-979b-738b6bc173ab_buggy.java b/tmp/933a1418-0ed8-4f65-a3f5-98d0beface08_fixed.java
index fd205cf..e697f3e 100644
--- a/tmp/cc402680-5e34-4298-979b-738b6bc173ab_buggy.java
+++ b/tmp/933a1418-0ed8-4f65-a3f5-98d0beface08_fixed.java
@@ -1,151 +1,163 @@
     private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {
 
         // If there's nothing there, treat it like we have no restrictions.
         if ((content == null) || (content.length == 0)) {
             return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
         }
 
         int bytesLen = content.length;
         int offset = 0;
-        Charset encoding = StandardCharsets.US_ASCII;
+        Charset encoding = StandardCharsets.UTF_8; // Default to UTF-8 per spec
 
+        // Check for BOMs and override encoding if present
+        boolean hasBOM = false;
         // Check for a UTF-8 BOM at the beginning (EF BB BF)
         if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {
             offset = 3;
             bytesLen -= 3;
             encoding = StandardCharsets.UTF_8;
+            hasBOM = true;
         }
         // Check for UTF-16LE BOM at the beginning (FF FE)
         else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {
             offset = 2;
             bytesLen -= 2;
             encoding = StandardCharsets.UTF_16LE;
+            hasBOM = true;
         }
         // Check for UTF-16BE BOM at the beginning (FE FF)
         else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {
             offset = 2;
             bytesLen -= 2;
             encoding = StandardCharsets.UTF_16BE;
+            hasBOM = true;
         }
 
-        String contentAsStr;
-        contentAsStr = new String(content, offset, bytesLen, encoding);
+        // If no BOM found, check Content-Type for charset
+        if (!hasBOM && contentType != null) {
+            String[] parts = contentType.split(";\\s*");
+            for (String part : parts) {
+                if (part.toLowerCase(Locale.ROOT).startsWith("charset=")) {
+                    String charsetName = part.substring("charset=".length()).trim();
+                    charsetName = charsetName.replaceAll("^['\"]|['\"]$", "");
+                    try {
+                        encoding = Charset.forName(charsetName);
+                    } catch (IllegalCharsetNameException | UnsupportedCharsetException e) {
+                        LOGGER.debug("Unsupported charset in content-type: {}", charsetName);
+                    }
+                    break;
+                }
+            }
+        }
+
+        String contentAsStr = new String(content, offset, bytesLen, encoding);
 
         // Decide if we need to do special HTML processing.
         boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith("text/html"));
 
-        // If it looks like it contains HTML, but doesn't have a user agent
-        // field, then
-        // assume somebody messed up and returned back to us a random HTML page
-        // instead
-        // of a robots.txt file.
+        // Check if content is HTML without user-agent field
         boolean hasHTML = false;
         if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {
             if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {
                 LOGGER.trace("Found non-robots.txt HTML file: " + url);
                 return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
             } else {
-                // We'll try to strip out HTML tags below.
-                if (isHtmlType) {
-                    LOGGER.debug("HTML content type returned for robots.txt file: " + url);
-                } else {
-                    LOGGER.debug("Found HTML in robots.txt file: " + url);
-                }
-
+                LOGGER.debug((isHtmlType ? "HTML content type returned for robots.txt file: " : "Found HTML in robots.txt file: ") + url);
                 hasHTML = true;
             }
         }
 
-        // Break on anything that might be used as a line ending. Since
-        // tokenizer doesn't return empty tokens, a \r\n sequence still
-        // works since it looks like an empty string between the \r and \n.
         StringTokenizer lineParser = new StringTokenizer(contentAsStr, "\n\r\u0085\u2028\u2029");
         ParseState parseState = new ParseState(url, robotNames);
 
         while (lineParser.hasMoreTokens()) {
             String line = lineParser.nextToken();
 
-            // Get rid of HTML markup, in case some brain-dead webmaster has
-            // created an HTML
-            // page for robots.txt. We could do more sophisticated processing
-            // here to better
-            // handle bad HTML, but that's a very tiny percentage of all
-            // robots.txt files.
             if (hasHTML) {
                 line = line.replaceAll("<[^>]+>", "");
             }
 
-            // trim out comments and whitespace
+            // Trim comments and whitespace
             int hashPos = line.indexOf("#");
             if (hashPos >= 0) {
                 line = line.substring(0, hashPos);
             }
-
             line = line.trim();
-            if (line.length() == 0) {
+            if (line.isEmpty()) {
                 continue;
             }
 
             RobotToken token = tokenize(line);
+            try {
                 switch (token.getDirective()) {
                     case USER_AGENT:
                         handleUserAgent(parseState, token);
                         break;
 
                     case DISALLOW:
                         parseState.setFinishedAgentFields(true);
-                handleDisallow(parseState, token);
+                        handleDisallow(parseState, decodeTokenValue(token));
                         break;
 
                     case ALLOW:
                         parseState.setFinishedAgentFields(true);
-                handleAllow(parseState, token);
+                        handleAllow(parseState, decodeTokenValue(token));
                         break;
 
                     case CRAWL_DELAY:
                         parseState.setFinishedAgentFields(true);
                         handleCrawlDelay(parseState, token);
                         break;
 
                     case SITEMAP:
                         parseState.setFinishedAgentFields(true);
                         handleSitemap(parseState, token);
                         break;
 
                     case HTTP:
                         parseState.setFinishedAgentFields(true);
                         handleHttp(parseState, token);
                         break;
 
                     case UNKNOWN:
                         reportWarning(parseState, "Unknown directive in robots.txt file: {}", line);
                         parseState.setFinishedAgentFields(true);
                         break;
 
                     case MISSING:
                         reportWarning(parseState, "Unknown line in robots.txt file (size {}): {}", content.length, line);
                         parseState.setFinishedAgentFields(true);
                         break;
 
                     default:
-                    // All others we just ignore
-                    // TODO KKr - which of these should be setting
-                    // finishedAgentFields to true?
-                    // TODO KKr - handle no-index
-                    // TODO KKr - handle request-rate and visit-time
+                        // Ignore other directives
                         break;
                 }
+            } catch (Exception e) {
+                reportWarning(parseState, "Error processing line: {}", line);
+            }
         }
 
         this._numWarningsDuringLastParse.set(parseState._numWarnings);
         SimpleRobotRules result = parseState.getRobotRules();
         if (result.getCrawlDelay() > _maxCrawlDelay) {
-            // Some evil sites use a value like 3600 (seconds) for the crawl
-            // delay, which would cause lots of problems for us.
-            LOGGER.debug("Crawl delay exceeds max value - so disallowing all URLs: {}", url);
+            LOGGER.debug("Crawl delay exceeds max value - disallowing all URLs: {}", url);
             return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);
         } else {
             result.sortRules();
             return result;
         }
     }
+
+    private RobotToken decodeTokenValue(RobotToken token) {
+        try {
+            String decodedValue = URLDecoder.decode(token.getValue(), StandardCharsets.UTF_8.name());
+            return new RobotToken(token.getDirective(), decodedValue);
+        } catch (UnsupportedEncodingException e) {
+            // Should not happen with UTF-8
+            return token;
+        } catch (IllegalArgumentException e) {
+            // Invalid percent encoding, return original token
+            return token;
+        }
+    }
\ No newline at end of file
