diff --git a/tmp/df308fef-ab58-4dd0-952f-71a7f7989642_buggy.java b/tmp/a755ec55-6a6c-4e89-9919-e655f66835a7_fixed.java
index fd205cf..88c5819 100644
--- a/tmp/df308fef-ab58-4dd0-952f-71a7f7989642_buggy.java
+++ b/tmp/a755ec55-6a6c-4e89-9919-e655f66835a7_fixed.java
@@ -1,151 +1,195 @@
+import crawlercommons.robots.SimpleRobotRules.RobotRulesMode;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.UnsupportedEncodingException;
+import java.net.URLDecoder;
+import java.nio.charset.Charset;
+import java.nio.charset.StandardCharsets;
+import java.util.Collection;
+import java.util.Locale;
+import java.util.StringTokenizer;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+// Assume necessary imports and surrounding class structure exist (e.g., fields like LOGGER, _numWarningsDuringLastParse, _maxCrawlDelay, patterns, handler methods, RobotToken, ParseState, etc.)
+// We only provide the fixed method as requested.
+
     private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {
 
         // If there's nothing there, treat it like we have no restrictions.
         if ((content == null) || (content.length == 0)) {
             return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
         }
 
         int bytesLen = content.length;
         int offset = 0;
-        Charset encoding = StandardCharsets.US_ASCII;
+        // Default to UTF-8 encoding per IETF draft standard (draft-koster-rep).
+        // This is the most common encoding for robots.txt and handles international characters.
+        Charset encoding = StandardCharsets.UTF_8;
 
-        // Check for a UTF-8 BOM at the beginning (EF BB BF)
+        // Check for UTF BOMs to override the default UTF-8.
+        // Check for a UTF-8 BOM at the beginning (EF BB BF). Note: BOM is discouraged in UTF-8, but we handle it.
         if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {
             offset = 3;
             bytesLen -= 3;
-            encoding = StandardCharsets.UTF_8;
+            // Encoding is already UTF-8, just adjust offset/length.
         }
         // Check for UTF-16LE BOM at the beginning (FF FE)
         else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {
             offset = 2;
             bytesLen -= 2;
             encoding = StandardCharsets.UTF_16LE;
         }
         // Check for UTF-16BE BOM at the beginning (FE FF)
         else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {
             offset = 2;
             bytesLen -= 2;
             encoding = StandardCharsets.UTF_16BE;
         }
+        // No BOM detected, proceed with the default (UTF-8).
 
         String contentAsStr;
+        try {
+             // Decode using the detected or default (UTF-8) encoding.
+             // Malformed sequences according to the charset will be replaced by the default replacement character.
             contentAsStr = new String(content, offset, bytesLen, encoding);
+        } catch (UnsupportedOperationException | IllegalArgumentException e) {
+            // Should not happen with standard charsets, but handle defensively.
+            LOGGER.warn("Could not decode robots.txt using encoding {} for url: {} - {}", encoding, url, e.getMessage());
+            // Treat as empty/allow all if decoding fails completely.
+            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
+        }
 
-        // Decide if we need to do special HTML processing.
+
+        // Decide if we need to do special HTML processing based on content type.
         boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith("text/html"));
 
-        // If it looks like it contains HTML, but doesn't have a user agent
-        // field, then
-        // assume somebody messed up and returned back to us a random HTML page
-        // instead
-        // of a robots.txt file.
+        // If it looks like it contains HTML tags (either by content type or basic pattern matching),
+        // check if it also contains a "user-agent" line. If not, assume it's an error page
+        // and not a real robots.txt file.
         boolean hasHTML = false;
+        // Note: SIMPLE_HTML_PATTERN is a basic check and might not be exhaustive.
         if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {
-            if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {
-                LOGGER.trace("Found non-robots.txt HTML file: " + url);
+            // USER_AGENT_PATTERN check should be case-insensitive.
+            Matcher userAgentMatcher = USER_AGENT_PATTERN.matcher(contentAsStr);
+            if (!userAgentMatcher.find()) {
+                // No User-agent found in potential HTML content.
+                LOGGER.trace("Found non-robots.txt HTML file (no User-agent): {}", url);
                 return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
             } else {
-                // We'll try to strip out HTML tags below.
+                // Found HTML, but also a User-agent line, so proceed with parsing but enable HTML stripping.
                 if (isHtmlType) {
-                    LOGGER.debug("HTML content type returned for robots.txt file: " + url);
+                    LOGGER.debug("HTML content type returned for robots.txt file: {}", url);
                 } else {
-                    LOGGER.debug("Found HTML in robots.txt file: " + url);
+                    LOGGER.debug("Found HTML tags in robots.txt file: {}", url);
                 }
-
                 hasHTML = true;
             }
         }
 
-        // Break on anything that might be used as a line ending. Since
-        // tokenizer doesn't return empty tokens, a \r\n sequence still
-        // works since it looks like an empty string between the \r and \n.
+        // Tokenize the content into logical lines based on standard line breaks.
+        // StringTokenizer skips empty tokens which works well for multiple line breaks.
         StringTokenizer lineParser = new StringTokenizer(contentAsStr, "\n\r\u0085\u2028\u2029");
-        ParseState parseState = new ParseState(url, robotNames);
+        // Initialize the state for parsing, passing necessary parameters.
+        ParseState parseState = new ParseState(url, robotNames, exactUserAgentMatching);
 
         while (lineParser.hasMoreTokens()) {
             String line = lineParser.nextToken();
 
-            // Get rid of HTML markup, in case some brain-dead webmaster has
-            // created an HTML
-            // page for robots.txt. We could do more sophisticated processing
-            // here to better
-            // handle bad HTML, but that's a very tiny percentage of all
-            // robots.txt files.
+            // If HTML was detected, attempt to strip basic tags before further processing.
             if (hasHTML) {
+                // This regex removes content between < and >. It's basic and might fail on complex/broken HTML.
                 line = line.replaceAll("<[^>]+>", "");
             }
 
-            // trim out comments and whitespace
+            // Remove comments starting with '#'.
             int hashPos = line.indexOf("#");
             if (hashPos >= 0) {
                 line = line.substring(0, hashPos);
             }
 
+            // Trim leading/trailing whitespace.
             line = line.trim();
-            if (line.length() == 0) {
+
+            // Skip lines that are now empty (e.g., were comments or whitespace only).
+            if (line.isEmpty()) {
                 continue;
             }
 
+            // Tokenize the line into a directive and its value (e.g., "Disallow: /path").
+            // Assumes tokenize handles case-insensitivity of the directive and extracts the value.
             RobotToken token = tokenize(line);
+
+            // Process the token based on the directive.
+            // Rules like Allow/Disallow apply to the *current* User-agent group being processed.
+            // Sitemaps are global. Unknown/invalid lines are ignored per spec.
             switch (token.getDirective()) {
                 case USER_AGENT:
+                    // handleUserAgent should update the current agent context within parseState.
                     handleUserAgent(parseState, token);
                     break;
 
                 case DISALLOW:
-                parseState.setFinishedAgentFields(true);
+                    // handleDisallow should add the rule (after percent-decoding the path)
+                    // to the current agent(s) being tracked by parseState.
                     handleDisallow(parseState, token);
                     break;
 
                 case ALLOW:
-                parseState.setFinishedAgentFields(true);
+                    // handleAllow should add the rule (after percent-decoding the path)
+                    // to the current agent(s) being tracked by parseState.
                     handleAllow(parseState, token);
                     break;
 
                 case CRAWL_DELAY:
-                parseState.setFinishedAgentFields(true);
+                    // handleCrawlDelay should set the delay for the current agent(s) in parseState.
                     handleCrawlDelay(parseState, token);
                     break;
 
                 case SITEMAP:
-                parseState.setFinishedAgentFields(true);
+                    // handleSitemap should add the sitemap URL (global context).
                     handleSitemap(parseState, token);
                     break;
 
                 case HTTP:
-                parseState.setFinishedAgentFields(true);
+                    // This directive is non-standard. The original code handled it; maintain behavior or log/ignore.
                     handleHttp(parseState, token);
                     break;
 
                 case UNKNOWN:
+                    // Log unknown directives. The specification requires ignoring them.
                     reportWarning(parseState, "Unknown directive in robots.txt file: {}", line);
-                parseState.setFinishedAgentFields(true);
                     break;
 
                 case MISSING:
-                reportWarning(parseState, "Unknown line in robots.txt file (size {}): {}", content.length, line);
-                parseState.setFinishedAgentFields(true);
+                    // Line doesn't conform to "directive: value". The specification requires ignoring it.
+                    reportWarning(parseState, "Ignoring line with unexpected format in robots.txt file (size {}): {}", content.length, line);
                     break;
 
                 default:
-                    // All others we just ignore
-                    // TODO KKr - which of these should be setting
-                    // finishedAgentFields to true?
-                    // TODO KKr - handle no-index
-                    // TODO KKr - handle request-rate and visit-time
+                    // Ignore any other directives encountered (e.g., Host, Noindex if not explicitly handled).
                     break;
             }
         }
 
+        // Finished processing all lines. Store the number of warnings encountered.
         this._numWarningsDuringLastParse.set(parseState._numWarnings);
+        // Retrieve the compiled rules from the parse state.
         SimpleRobotRules result = parseState.getRobotRules();
+
+        // Enforce a maximum crawl delay. If the parsed delay exceeds the limit,
+        // treat it as if the site disallows everything (for the matched agent).
         if (result.getCrawlDelay() > _maxCrawlDelay) {
-            // Some evil sites use a value like 3600 (seconds) for the crawl
-            // delay, which would cause lots of problems for us.
-            LOGGER.debug("Crawl delay exceeds max value - so disallowing all URLs: {}", url);
+            // Note: Returning a generic ALLOW_NONE might discard sitemap information.
+            // A more nuanced approach could modify the existing rules, but we follow the original logic.
+            LOGGER.debug("Crawl delay ({}) for {} exceeds max ({}) - disallowing all URLs: {}",
+                         result.getCrawlDelay(), parseState.activeAgent(), _maxCrawlDelay, url); // Assumes parseState.activeAgent() gives relevant agent name
             return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);
         } else {
+            // Sort the rules (Allow/Disallow) for potentially faster matching.
             result.sortRules();
             return result;
         }
     }
\ No newline at end of file
