diff --git a/tmp/999389a6-a351-4b81-b4e9-c51d38dc119f_buggy.java b/tmp/0152c96f-6ac4-41d4-9b66-3ed61a3fd2f0_fixed.java
index fd205cf..e46026b 100644
--- a/tmp/999389a6-a351-4b81-b4e9-c51d38dc119f_buggy.java
+++ b/tmp/0152c96f-6ac4-41d4-9b66-3ed61a3fd2f0_fixed.java
@@ -1,151 +1,197 @@
     private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {
 
         // If there's nothing there, treat it like we have no restrictions.
         if ((content == null) || (content.length == 0)) {
             return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
         }
 
         int bytesLen = content.length;
         int offset = 0;
-        Charset encoding = StandardCharsets.US_ASCII;
+        // Per Google spec, default encoding is UTF-8
+        // https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt#file-format
+        Charset encoding = StandardCharsets.UTF_8;
 
         // Check for a UTF-8 BOM at the beginning (EF BB BF)
         if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {
             offset = 3;
             bytesLen -= 3;
-            encoding = StandardCharsets.UTF_8;
+            // Encoding is already UTF-8
         }
         // Check for UTF-16LE BOM at the beginning (FF FE)
         else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {
             offset = 2;
             bytesLen -= 2;
             encoding = StandardCharsets.UTF_16LE;
         }
         // Check for UTF-16BE BOM at the beginning (FE FF)
         else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {
             offset = 2;
             bytesLen -= 2;
             encoding = StandardCharsets.UTF_16BE;
         }
+        // Although not standard, check for UTF-32LE BOM (FF FE 00 00)
+        else if ((bytesLen >= 4) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE) && (content[2] == (byte) 0x00) && (content[3] == (byte) 0x00)) {
+            offset = 4;
+            bytesLen -= 4;
+            // Java doesn't have a standard Charset name for UTF_32LE?
+            // Use available name if possible, otherwise stick to UTF-8 as fallback
+            try {
+                encoding = Charset.forName("UTF-32LE");
+            } catch (UnsupportedCharsetException e) {
+                LOGGER.warn("UTF-32LE BOM detected, but Charset is not supported. Falling back to UTF-8 for URL: {}", url);
+                encoding = StandardCharsets.UTF_8; // Fallback, though likely incorrect.
+                // Reset offset/bytesLen as we can't decode it properly without the charset.
+                offset = 0;
+                bytesLen = content.length;
+            }
+        }
+        // Although not standard, check for UTF-32BE BOM (00 00 FE FF)
+        else if ((bytesLen >= 4) && (content[0] == (byte) 0x00) && (content[1] == (byte) 0x00) && (content[2] == (byte) 0xFE) && (content[3] == (byte) 0xFF)) {
+            offset = 4;
+            bytesLen -= 4;
+            try {
+                encoding = Charset.forName("UTF-32BE");
+            } catch (UnsupportedCharsetException e) {
+                 LOGGER.warn("UTF-32BE BOM detected, but Charset is not supported. Falling back to UTF-8 for URL: {}", url);
+                 encoding = StandardCharsets.UTF_8; // Fallback
+                 offset = 0;
+                 bytesLen = content.length;
+            }
+        }
+
 
         String contentAsStr;
+        try {
+            // MalformedInputException or UnmappableCharacterException can occur
+            // If decoding fails, treat as empty as we can't parse reliably.
             contentAsStr = new String(content, offset, bytesLen, encoding);
+        } catch (Exception e) {
+             LOGGER.warn("Failed to decode robots.txt using charset {}. Treating as empty for URL: {}", encoding, url, e);
+             return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
+        }
+
 
         // Decide if we need to do special HTML processing.
         boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith("text/html"));
 
         // If it looks like it contains HTML, but doesn't have a user agent
         // field, then
         // assume somebody messed up and returned back to us a random HTML page
         // instead
         // of a robots.txt file.
         boolean hasHTML = false;
         if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {
             if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {
                 LOGGER.trace("Found non-robots.txt HTML file: " + url);
                 return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
             } else {
                 // We'll try to strip out HTML tags below.
                 if (isHtmlType) {
                     LOGGER.debug("HTML content type returned for robots.txt file: " + url);
                 } else {
                     LOGGER.debug("Found HTML in robots.txt file: " + url);
                 }
 
                 hasHTML = true;
             }
         }
 
         // Break on anything that might be used as a line ending. Since
         // tokenizer doesn't return empty tokens, a \r\n sequence still
         // works since it looks like an empty string between the \r and \n.
-        StringTokenizer lineParser = new StringTokenizer(contentAsStr, "\n\r\u0085\u2028\u2029");
-        ParseState parseState = new ParseState(url, robotNames);
-
-        while (lineParser.hasMoreTokens()) {
-            String line = lineParser.nextToken();
+        // Using split with a regex is generally safer for handling various line endings correctly.
+        // StringTokenizer might miss empty lines depending on the sequence of delimiters.
+        // Using split("[\n\r\u0085\u2028\u2029]+") handles multiple consecutive breaks better.
+        String[] lines = contentAsStr.split("[\n\r\u0085\u2028\u2029]+");
+        ParseState parseState = new ParseState(url, robotNames, exactUserAgentMatching); // Pass exactUserAgentMatching to ParseState constructor
 
+        for (String line : lines) {
              // Get rid of HTML markup, in case some brain-dead webmaster has
-            // created an HTML
-            // page for robots.txt. We could do more sophisticated processing
-            // here to better
-            // handle bad HTML, but that's a very tiny percentage of all
-            // robots.txt files.
+             // created an HTML page for robots.txt.
              if (hasHTML) {
+                 // Basic removal, might not cover all edge cases like tags spanning lines (though split helps).
                  line = line.replaceAll("<[^>]+>", "");
              }
 
              // trim out comments and whitespace
              int hashPos = line.indexOf("#");
              if (hashPos >= 0) {
                  line = line.substring(0, hashPos);
              }
 
              line = line.trim();
              if (line.length() == 0) {
                  continue;
              }
 
              RobotToken token = tokenize(line);
              switch (token.getDirective()) {
                  case USER_AGENT:
                  handleUserAgent(parseState, token);
                      break;
 
                  case DISALLOW:
                  parseState.setFinishedAgentFields(true);
                  handleDisallow(parseState, token);
                      break;
 
                  case ALLOW:
                  parseState.setFinishedAgentFields(true);
                  handleAllow(parseState, token);
                      break;
 
                  case CRAWL_DELAY:
                  parseState.setFinishedAgentFields(true);
                  handleCrawlDelay(parseState, token);
                      break;
 
                  case SITEMAP:
-                parseState.setFinishedAgentFields(true);
+                 // Sitemaps are not agent-specific, but processing them after agent fields seems reasonable.
+                 // Google treats them as independent of any agent block.
+                 // parseState.setFinishedAgentFields(true); // This might be incorrect depending on desired grouping behavior. Let ParseState handle it.
                  handleSitemap(parseState, token);
                      break;
 
                  case HTTP:
                  parseState.setFinishedAgentFields(true);
                  handleHttp(parseState, token);
                      break;
 
                  case UNKNOWN:
-                reportWarning(parseState, "Unknown directive in robots.txt file: {}", line);
-                parseState.setFinishedAgentFields(true);
+                 // Per Google Spec, ignore unknown directives. Log as trace or debug instead of warning.
+                 LOGGER.trace("Unknown directive in robots.txt file: {} in {}", line, url);
+                 // parseState.setFinishedAgentFields(true); // Unknown directives shouldn't affect agent grouping.
                      break;
 
                  case MISSING:
-                reportWarning(parseState, "Unknown line in robots.txt file (size {}): {}", content.length, line);
-                parseState.setFinishedAgentFields(true);
+                 // This typically means a line couldn't be parsed into field:value. Ignore per Google spec.
+                 LOGGER.trace("Ignoring unparseable line in robots.txt file (size {}): {} in {}", content.length, line, url);
+                 // parseState.setFinishedAgentFields(true); // Unparseable lines shouldn't affect agent grouping.
                      break;
 
                  default:
-                    // All others we just ignore
-                    // TODO KKr - which of these should be setting
-                    // finishedAgentFields to true?
+                     // Includes HOST, NOINDEX, REQUEST_RATE, VISIT_TIME etc. which might be handled specifically or ignored.
+                     // Currently ignored. Setting finishedAgentFields depends on the directive.
+                     // For now, assume they don't impact agent grouping unless handled specifically.
                      // TODO KKr - handle no-index
                      // TODO KKr - handle request-rate and visit-time
                      break;
              }
          }
 
+
         this._numWarningsDuringLastParse.set(parseState._numWarnings);
         SimpleRobotRules result = parseState.getRobotRules();
         if (result.getCrawlDelay() > _maxCrawlDelay) {
             // Some evil sites use a value like 3600 (seconds) for the crawl
             // delay, which would cause lots of problems for us.
             LOGGER.debug("Crawl delay exceeds max value - so disallowing all URLs: {}", url);
-            return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);
+            // Create rules disallowing everything but retain sitemaps and other global rules.
+            SimpleRobotRules restrictedRules = new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);
+            restrictedRules.copyGlobals(result); // Copy sitemaps etc.
+            return restrictedRules;
         } else {
             result.sortRules();
             return result;
         }
     }
\ No newline at end of file
