diff --git a/tmp/a82a34fb-53d4-4993-a136-45df39db8885_buggy.java b/tmp/33177782-c749-48a7-9092-6e0e7afe4d71_fixed.java
index fd205cf..1036fde 100644
--- a/tmp/a82a34fb-53d4-4993-a136-45df39db8885_buggy.java
+++ b/tmp/33177782-c749-48a7-9092-6e0e7afe4d71_fixed.java
@@ -1,151 +1,250 @@
+import java.nio.ByteBuffer;
+import java.nio.charset.CharacterCodingException;
+import java.nio.charset.Charset;
+import java.nio.charset.CharsetDecoder;
+import java.nio.charset.CodingErrorAction;
+import java.nio.charset.StandardCharsets;
+import java.util.Collection;
+import java.util.Locale;
+import java.util.StringTokenizer;
+
+// Assuming other necessary classes like SimpleRobotRules, RobotRulesMode, RobotToken, ParseState, LOGGER,
+// SIMPLE_HTML_PATTERN, USER_AGENT_PATTERN, _numWarningsDuringLastParse, _maxCrawlDelay,
+// tokenize, handleUserAgent, handleDisallow, handleAllow, handleCrawlDelay, handleSitemap,
+// handleHttp, reportWarning are available in the scope.
+
+class YourClassName { // Replace YourClassName with the actual class name containing the function
+
+    // Assuming these are defined elsewhere in the class or accessible static members
+    // private static final Logger LOGGER = LoggerFactory.getLogger(YourClassName.class);
+    // private static final Pattern SIMPLE_HTML_PATTERN = Pattern.compile("<html", Pattern.CASE_INSENSITIVE);
+    // private static final Pattern USER_AGENT_PATTERN = Pattern.compile("user-agent", Pattern.CASE_INSENSITIVE);
+    // private final AtomicInteger _numWarningsDuringLastParse = new AtomicInteger(0);
+    // private int _maxCrawlDelay = 3600; // Example default, adjust as per actual class definition
+
+    // Mock/placeholder for missing context, to make the snippet runnable if standalone
+    // Replace with actual imports/definitions from the original project if providing a full class.
+    static class SimpleRobotRules {
+        enum RobotRulesMode { ALLOW_ALL, ALLOW_NONE }
+        public SimpleRobotRules(RobotRulesMode mode) {}
+        public SimpleRobotRules() {}
+        public long getCrawlDelay() { return 0; }
+        public void sortRules() {}
+        public boolean isAllowed(String url) { return true; } // Mock for compilation
+    }
+    static class RobotToken {
+        enum Directive { USER_AGENT, DISALLOW, ALLOW, CRAWL_DELAY, SITEMAP, HTTP, UNKNOWN, MISSING }
+        public Directive getDirective() { return Directive.UNKNOWN; }
+        public String getAgent() { return ""; }
+        public String getPath() { return ""; }
+        public String getUrl() { return ""; }
+        public long getCrawlDelay() { return 0; }
+    }
+    static class ParseState {
+        int _numWarnings = 0;
+        public ParseState(String url, Collection<String> robotNames) {}
+        public void setFinishedAgentFields(boolean finished) {}
+        public SimpleRobotRules getRobotRules() { return new SimpleRobotRules(); }
+    }
+    // Assume LOGGER is available from a logging framework like SLF4J
+    static class LOGGER {
+        static void trace(String msg) {}
+        static void debug(String msg) {}
+        static void debug(String msg, Object... args) {}
+    }
+    static java.util.regex.Pattern SIMPLE_HTML_PATTERN = java.util.regex.Pattern.compile("<html", java.util.regex.Pattern.CASE_INSENSITIVE);
+    static java.util.regex.Pattern USER_AGENT_PATTERN = java.util.regex.Pattern.compile("user-agent", java.util.regex.Pattern.CASE_INSENSITIVE);
+    private final java.util.concurrent.atomic.AtomicInteger _numWarningsDuringLastParse = new java.util.concurrent.atomic.AtomicInteger(0);
+    private int _maxCrawlDelay = 3600;
+
+    private RobotToken tokenize(String line) { return new RobotToken(); } // Mock
+    private void handleUserAgent(ParseState parseState, RobotToken token) {} // Mock
+    private void handleDisallow(ParseState parseState, RobotToken token) {} // Mock
+    private void handleAllow(ParseState parseState, RobotToken token) {} // Mock
+    private void handleCrawlDelay(ParseState parseState, RobotToken token) {} // Mock
+    private void handleSitemap(ParseState parseState, RobotToken token) {} // Mock
+    private void handleHttp(ParseState parseState, RobotToken token) {} // Mock
+    private void reportWarning(ParseState parseState, String format, Object... args) {} // Mock
+
     private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {
 
         // If there's nothing there, treat it like we have no restrictions.
         if ((content == null) || (content.length == 0)) {
             return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
         }
 
         int bytesLen = content.length;
         int offset = 0;
-        Charset encoding = StandardCharsets.US_ASCII;
+        Charset encoding = null; // Start with null, will be determined by BOM or fallback
 
         // Check for a UTF-8 BOM at the beginning (EF BB BF)
         if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {
             offset = 3;
             bytesLen -= 3;
             encoding = StandardCharsets.UTF_8;
         }
         // Check for UTF-16LE BOM at the beginning (FF FE)
         else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {
             offset = 2;
             bytesLen -= 2;
             encoding = StandardCharsets.UTF_16LE;
         }
         // Check for UTF-16BE BOM at the beginning (FE FF)
         else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {
             offset = 2;
             bytesLen -= 2;
             encoding = StandardCharsets.UTF_16BE;
         }
 
         String contentAsStr;
+        // If no BOM was found, try common encodings. RFC 9309 mandates UTF-8.
+        if (encoding == null) {
+            // First try UTF-8, as it's the standard and most common for non-ASCII.
+            try {
+                CharsetDecoder decoder = StandardCharsets.UTF_8.newDecoder()
+                        .onMalformedInput(CodingErrorAction.REPORT)
+                        .onUnmappableCharacter(CodingErrorAction.REPORT);
+                contentAsStr = decoder.decode(ByteBuffer.wrap(content, offset, bytesLen)).toString();
+                encoding = StandardCharsets.UTF_8; // Mark that UTF-8 was successfully used
+            } catch (CharacterCodingException e) {
+                // If UTF-8 fails (malformed input), try ISO-8859-1 as a common fallback for legacy European content.
+                try {
+                    CharsetDecoder decoder = StandardCharsets.ISO_8859_1.newDecoder()
+                            .onMalformedInput(CodingErrorAction.REPORT)
+                            .onUnmappableCharacter(CodingErrorAction.REPORT);
+                    contentAsStr = decoder.decode(ByteBuffer.wrap(content, offset, bytesLen)).toString();
+                    encoding = StandardCharsets.ISO_8859_1; // Mark that ISO-8859-1 was successfully used
+                } catch (CharacterCodingException e2) {
+                    // If all strict decodings fail, fall back to US_ASCII and let it replace bad characters.
+                    // This is the least preferred but most robust fallback, ensuring parsing continues.
+                    LOGGER.debug("Could not decode robots.txt with UTF-8 or ISO-8859-1; falling back to US-ASCII (replacement characters will be used): {}", url);
+                    contentAsStr = new String(content, offset, bytesLen, StandardCharsets.US_ASCII);
+                    encoding = StandardCharsets.US_ASCII; // Mark the fallback encoding
+                }
+            }
+        } else {
+            // A BOM was detected, use that encoding directly.
+            // The String constructor will replace malformed characters if any, which is acceptable behavior
+            // as BOMs are strong indicators of the intended encoding.
             contentAsStr = new String(content, offset, bytesLen, encoding);
+        }
 
         // Decide if we need to do special HTML processing.
         boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith("text/html"));
 
         // If it looks like it contains HTML, but doesn't have a user agent
         // field, then
         // assume somebody messed up and returned back to us a random HTML page
         // instead
         // of a robots.txt file.
         boolean hasHTML = false;
         if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {
             if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {
                 LOGGER.trace("Found non-robots.txt HTML file: " + url);
                 return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
             } else {
                 // We'll try to strip out HTML tags below.
                 if (isHtmlType) {
                     LOGGER.debug("HTML content type returned for robots.txt file: " + url);
                 } else {
                     LOGGER.debug("Found HTML in robots.txt file: " + url);
                 }
 
                 hasHTML = true;
             }
         }
 
         // Break on anything that might be used as a line ending. Since
         // tokenizer doesn't return empty tokens, a \r\n sequence still
         // works since it looks like an empty string between the \r and \n.
         StringTokenizer lineParser = new StringTokenizer(contentAsStr, "\n\r\u0085\u2028\u2029");
         ParseState parseState = new ParseState(url, robotNames);
 
         while (lineParser.hasMoreTokens()) {
             String line = lineParser.nextToken();
 
             // Get rid of HTML markup, in case some brain-dead webmaster has
             // created an HTML
             // page for robots.txt. We could do more sophisticated processing
             // here to better
             // handle bad HTML, but that's a very tiny percentage of all
             // robots.txt files.
             if (hasHTML) {
                 line = line.replaceAll("<[^>]+>", "");
             }
 
             // trim out comments and whitespace
             int hashPos = line.indexOf("#");
             if (hashPos >= 0) {
                 line = line.substring(0, hashPos);
             }
 
             line = line.trim();
             if (line.length() == 0) {
                 continue;
             }
 
             RobotToken token = tokenize(line);
             switch (token.getDirective()) {
                 case USER_AGENT:
                 handleUserAgent(parseState, token);
                     break;
 
                 case DISALLOW:
                 parseState.setFinishedAgentFields(true);
                 handleDisallow(parseState, token);
                     break;
 
                 case ALLOW:
                 parseState.setFinishedAgentFields(true);
                 handleAllow(parseState, token);
                     break;
 
                 case CRAWL_DELAY:
                 parseState.setFinishedAgentFields(true);
                 handleCrawlDelay(parseState, token);
                     break;
 
                 case SITEMAP:
                 parseState.setFinishedAgentFields(true);
                 handleSitemap(parseState, token);
                     break;
 
                 case HTTP:
                 parseState.setFinishedAgentFields(true);
                 handleHttp(parseState, token);
                     break;
 
                 case UNKNOWN:
                 reportWarning(parseState, "Unknown directive in robots.txt file: {}", line);
                 parseState.setFinishedAgentFields(true);
                     break;
 
                 case MISSING:
                 reportWarning(parseState, "Unknown line in robots.txt file (size {}): {}", content.length, line);
                 parseState.setFinishedAgentFields(true);
                     break;
 
                 default:
                     // All others we just ignore
                     // TODO KKr - which of these should be setting
                     // finishedAgentFields to true?
                     // TODO KKr - handle no-index
                     // TODO KKr - handle request-rate and visit-time
                     break;
             }
         }
 
         this._numWarningsDuringLastParse.set(parseState._numWarnings);
         SimpleRobotRules result = parseState.getRobotRules();
         if (result.getCrawlDelay() > _maxCrawlDelay) {
             // Some evil sites use a value like 3600 (seconds) for the crawl
             // delay, which would cause lots of problems for us.
             LOGGER.debug("Crawl delay exceeds max value - so disallowing all URLs: {}", url);
             return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);
         } else {
             result.sortRules();
             return result;
         }
     }
+}
\ No newline at end of file
