diff --git a/tmp/b2688eb4-e83b-404e-8141-7509584151bf_buggy.java b/tmp/4a2594a7-b930-4d17-bf7b-f276b0c8cb72_fixed.java
index fd205cf..5a1c045 100644
--- a/tmp/b2688eb4-e83b-404e-8141-7509584151bf_buggy.java
+++ b/tmp/4a2594a7-b930-4d17-bf7b-f276b0c8cb72_fixed.java
@@ -1,151 +1,241 @@
+import java.nio.ByteBuffer;
+import java.nio.charset.CharacterCodingException;
+import java.nio.charset.Charset;
+import java.nio.charset.CharsetDecoder;
+import java.nio.charset.CodingErrorAction;
+import java.nio.charset.StandardCharsets;
+import java.util.Collection;
+import java.util.Locale;
+import java.util.StringTokenizer;
+import java.util.regex.Pattern;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+// Assume relevant imports and static fields are available from the original class context.
+// For completeness, adding the necessary ones here if not already imported in the environment.
+// For example:
+// import crawlercommons.robots.SimpleRobotRules;
+// import crawlercommons.robots.SimpleRobotRules.RobotRulesMode;
+// import crawlercommons.robots.BaseRobotRules; // For SimpleRobotRules class
+// import crawlercommons.robots.ParsedUrlGroup.RobotToken; // For RobotToken enum
+
+// Dummy definitions for types not included in the snippet, assume they exist in the actual project
+class SimpleRobotRules {
+    enum RobotRulesMode { ALLOW_ALL, ALLOW_NONE }
+    private long crawlDelay;
+    public SimpleRobotRules(RobotRulesMode mode) {}
+    public long getCrawlDelay() { return crawlDelay; }
+    public void sortRules() {}
+}
+
+class ParseState {
+    int _numWarnings;
+    public ParseState(String url, Collection<String> robotNames) {}
+    public void setFinishedAgentFields(boolean finishedAgentFields) {}
+    public SimpleRobotRules getRobotRules() { return new SimpleRobotRules(SimpleRobotRules.RobotRulesMode.ALLOW_ALL); } // Dummy
+}
+
+class RobotToken {
+    enum Directive { USER_AGENT, DISALLOW, ALLOW, CRAWL_DELAY, SITEMAP, HTTP, UNKNOWN, MISSING }
+    public Directive getDirective() { return Directive.UNKNOWN; } // Dummy
+}
+
+public class SimpleRobotRulesParser {
+
+    private static final Logger LOGGER = LoggerFactory.getLogger(SimpleRobotRulesParser.class);
+    private static final Pattern SIMPLE_HTML_PATTERN = Pattern.compile("<html", Pattern.CASE_INSENSITIVE);
+    private static final Pattern USER_AGENT_PATTERN = Pattern.compile("User-agent", Pattern.CASE_INSENSITIVE);
+    private final long _maxCrawlDelay = Long.MAX_VALUE; // Dummy value
+
+    // Dummy method implementations for the context
+    private RobotToken tokenize(String line) { return new RobotToken(); }
+    private void handleUserAgent(ParseState parseState, RobotToken token) {}
+    private void handleDisallow(ParseState parseState, RobotToken token) {}
+    private void handleAllow(ParseState parseState, RobotToken token) {}
+    private void handleCrawlDelay(ParseState parseState, RobotToken token) {}
+    private void handleSitemap(ParseState parseState, RobotToken token) {}
+    private void handleHttp(ParseState parseState, RobotToken token) {}
+    private void reportWarning(ParseState parseState, String format, Object... args) { parseState._numWarnings++; }
+
+    // This field needs to be defined in the actual class
+    private final java.util.concurrent.atomic.AtomicInteger _numWarningsDuringLastParse = new java.util.concurrent.atomic.AtomicInteger(0);
+
+
     private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {
 
         // If there's nothing there, treat it like we have no restrictions.
         if ((content == null) || (content.length == 0)) {
-            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
+            return new SimpleRobotRules(SimpleRobotRules.RobotRulesMode.ALLOW_ALL);
         }
 
         int bytesLen = content.length;
         int offset = 0;
-        Charset encoding = StandardCharsets.US_ASCII;
+        Charset encoding = null; // Changed to null, will be determined below
 
         // Check for a UTF-8 BOM at the beginning (EF BB BF)
         if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {
             offset = 3;
             bytesLen -= 3;
             encoding = StandardCharsets.UTF_8;
         }
         // Check for UTF-16LE BOM at the beginning (FF FE)
         else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {
             offset = 2;
             bytesLen -= 2;
             encoding = StandardCharsets.UTF_16LE;
         }
         // Check for UTF-16BE BOM at the beginning (FE FF)
         else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {
             offset = 2;
             bytesLen -= 2;
             encoding = StandardCharsets.UTF_16BE;
         }
 
         String contentAsStr;
+        // If no BOM was found, try to determine encoding (UTF-8, then ISO-8859-1 fallback)
+        if (encoding == null) {
+            ByteBuffer byteBuffer = ByteBuffer.wrap(content, offset, bytesLen);
+            CharsetDecoder utf8Decoder = StandardCharsets.UTF_8.newDecoder()
+                                               .onMalformedInput(CodingErrorAction.REPORT)
+                                               .onUnmappableCharacter(CodingErrorAction.REPORT);
+            CharsetDecoder iso88591Decoder = StandardCharsets.ISO_8859_1.newDecoder()
+                                                 .onMalformedInput(CodingErrorAction.REPLACE) // Allow replacement for malformed input
+                                                 .onUnmappableCharacter(CodingErrorAction.REPLACE); // Allow replacement for unmappable characters
+
+            try {
+                // Try UTF-8 first for content without BOM
+                contentAsStr = utf8Decoder.decode(byteBuffer).toString();
+                encoding = StandardCharsets.UTF_8; // Record the successful encoding
+            } catch (CharacterCodingException e) {
+                // If UTF-8 decoding fails (e.g., malformed sequence), fall back to ISO-8859-1
+                // Reset buffer position before trying another decoder
+                byteBuffer.rewind();
+                LOGGER.debug("UTF-8 decoding failed for robots.txt ({}). Falling back to ISO-8859-1. Error: {}", url, e.getMessage());
+                contentAsStr = iso88591Decoder.decode(byteBuffer).toString();
+                encoding = StandardCharsets.ISO_8859_1; // Record the successful encoding
+            }
+        } else {
+            // Use the encoding determined by BOM (already set)
             contentAsStr = new String(content, offset, bytesLen, encoding);
+        }
 
         // Decide if we need to do special HTML processing.
         boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith("text/html"));
 
         // If it looks like it contains HTML, but doesn't have a user agent
         // field, then
         // assume somebody messed up and returned back to us a random HTML page
         // instead
         // of a robots.txt file.
         boolean hasHTML = false;
         if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {
             if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {
                 LOGGER.trace("Found non-robots.txt HTML file: " + url);
-                return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);
+                return new SimpleRobotRules(SimpleRobotRules.RobotRulesMode.ALLOW_ALL);
             } else {
                 // We'll try to strip out HTML tags below.
                 if (isHtmlType) {
                     LOGGER.debug("HTML content type returned for robots.txt file: " + url);
                 } else {
                     LOGGER.debug("Found HTML in robots.txt file: " + url);
                 }
 
                 hasHTML = true;
             }
         }
 
         // Break on anything that might be used as a line ending. Since
         // tokenizer doesn't return empty tokens, a \r\n sequence still
         // works since it looks like an empty string between the \r and \n.
         StringTokenizer lineParser = new StringTokenizer(contentAsStr, "\n\r\u0085\u2028\u2029");
         ParseState parseState = new ParseState(url, robotNames);
 
         while (lineParser.hasMoreTokens()) {
             String line = lineParser.nextToken();
 
             // Get rid of HTML markup, in case some brain-dead webmaster has
             // created an HTML
             // page for robots.txt. We could do more sophisticated processing
             // here to better
             // handle bad HTML, but that's a very tiny percentage of all
             // robots.txt files.
             if (hasHTML) {
                 line = line.replaceAll("<[^>]+>", "");
             }
 
             // trim out comments and whitespace
             int hashPos = line.indexOf("#");
             if (hashPos >= 0) {
                 line = line.substring(0, hashPos);
             }
 
             line = line.trim();
             if (line.length() == 0) {
                 continue;
             }
 
             RobotToken token = tokenize(line);
             switch (token.getDirective()) {
                 case USER_AGENT:
                 handleUserAgent(parseState, token);
                     break;
 
                 case DISALLOW:
                 parseState.setFinishedAgentFields(true);
                 handleDisallow(parseState, token);
                     break;
 
                 case ALLOW:
                 parseState.setFinishedAgentFields(true);
                 handleAllow(parseState, token);
                     break;
 
                 case CRAWL_DELAY:
                 parseState.setFinishedAgentFields(true);
                 handleCrawlDelay(parseState, token);
                     break;
 
                 case SITEMAP:
                 parseState.setFinishedAgentFields(true);
                 handleSitemap(parseState, token);
                     break;
 
                 case HTTP:
                 parseState.setFinishedAgentFields(true);
                 handleHttp(parseState, token);
                     break;
 
                 case UNKNOWN:
                 reportWarning(parseState, "Unknown directive in robots.txt file: {}", line);
                 parseState.setFinishedAgentFields(true);
                     break;
 
                 case MISSING:
                 reportWarning(parseState, "Unknown line in robots.txt file (size {}): {}", content.length, line);
                 parseState.setFinishedAgentFields(true);
                     break;
 
                 default:
                     // All others we just ignore
                     // TODO KKr - which of these should be setting
                     // finishedAgentFields to true?
                     // TODO KKr - handle no-index
                     // TODO KKr - handle request-rate and visit-time
                     break;
             }
         }
 
         this._numWarningsDuringLastParse.set(parseState._numWarnings);
         SimpleRobotRules result = parseState.getRobotRules();
         if (result.getCrawlDelay() > _maxCrawlDelay) {
             // Some evil sites use a value like 3600 (seconds) for the crawl
             // delay, which would cause lots of problems for us.
             LOGGER.debug("Crawl delay exceeds max value - so disallowing all URLs: {}", url);
-            return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);
+            return new SimpleRobotRules(SimpleRobotRules.RobotRulesMode.ALLOW_NONE);
         } else {
             result.sortRules();
             return result;
         }
     }
+}
\ No newline at end of file
